{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19514,"status":"ok","timestamp":1707921977847,"user":{"displayName":"田口采樹","userId":"02806376211526518224"},"user_tz":-540},"id":"CnJzt1NZ4EWY","outputId":"020142d5-e471-4a94-e914-04f47d72976b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')  #listになかったのでドライブ上にデータ保存し、ドライブをマウント。"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9191,"status":"ok","timestamp":1707921987034,"user":{"displayName":"田口采樹","userId":"02806376211526518224"},"user_tz":-540},"id":"GcsnZUuF9VPq","outputId":"46d25c09-7352-4930-d2ed-caa5e5a5ce66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-tabnet\n","  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.25.2)\n","Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.2.2)\n","Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.11.4)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.1.0+cu121)\n","Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n","Installing collected packages: pytorch-tabnet\n","Successfully installed pytorch-tabnet-4.1.0\n"]}],"source":["! pip install pytorch-tabnet"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"-KH7RQho3diA","executionInfo":{"status":"ok","timestamp":1707921987035,"user_tz":-540,"elapsed":4,"user":{"displayName":"田口采樹","userId":"02806376211526518224"}}},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/signate/モジュール系/EDA')\n","sys.path.append('/content/drive/MyDrive/signate/モジュール系/Feature Engineering')\n","sys.path.append('/content/drive/MyDrive/signate/モジュール系/meanF1')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"MMrvEUSI3fPB","executionInfo":{"status":"ok","timestamp":1707921993221,"user_tz":-540,"elapsed":6189,"user":{"displayName":"田口采樹","userId":"02806376211526518224"}}},"outputs":[],"source":["### 自作モジュール\n","from plot_qq_plots import plot_qq_plots\n","from reduce_mem import reduce_mem_usage\n","from show_corr import show_corr\n","from target_encoded import get_kfold, get_targetencoding, to_target\n","from obj_to_cat import to_cat_data\n","from normalize_col import normalize_columns\n","from combined_features import create_combined_features\n","from threshold_tuning_binary import threshold_tuning_binary"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KyHlYQsK3zBa","executionInfo":{"status":"ok","timestamp":1707921999405,"user_tz":-540,"elapsed":6185,"user":{"displayName":"田口采樹","userId":"02806376211526518224"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","from pytorch_tabnet.pretraining import TabNetPretrainer\n","from pytorch_tabnet.tab_model import TabNetClassifier\n","import torch\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5147,"status":"ok","timestamp":1707922004550,"user":{"displayName":"田口采樹","userId":"02806376211526518224"},"user_tz":-540},"id":"81v7Xjx74B07","outputId":"70048926-e80c-460c-82df-32ddd553089c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Memory usage of dataframe is 13.23 MB\n","Memory usage after optimization is: 2.95 MB\n","Decreased by 77.7%\n","Memory usage of dataframe is 12.91 MB\n","Memory usage after optimization is: 2.91 MB\n","Decreased by 77.5%\n"]}],"source":["feat = \"feat00\"\n","train = pd.read_csv(f\"/content/drive/MyDrive/signate/債務不履行/途中経過_csv/feat_train_{feat}.csv\")\n","test = pd.read_csv(f\"/content/drive/MyDrive/signate/債務不履行/途中経過_csv/feat_test_{feat}.csv\")\n","train_origin = pd.read_csv(\"/content/drive/MyDrive/signate/債務不履行/データセット/train.csv\")\n","\n","train = reduce_mem_usage(train)\n","test = reduce_mem_usage(test)\n","\n","# 説明変数のカラム\n","cols_exp = [c for c in test.columns]\n","\n","###-----------------------------------------------------------------------------\n","## カテゴリ変数の抽出\n","cols_notcat = ['MIS_Status', 'Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob',\n","               'DisbursementGross', 'GrAppv', 'SBA_Appv','population', 'density',\n","               'lr_avg_ppi','ApprovalToDisbursement_days','outlier'] # 数値特徴量 'DisbursementGross_minus_SBA_Appv'\n","# county_col = ['county_gdp','sector_count']\n","time_col = [\"DisbursementDate_timestamp\",\t\"ApprovalDate_timestamp\",\t\"LoanEnd_timestamp\"]\n","dissba = ['DisbursementGross_plus_SBA_Appv','DisbursementGross_minus_SBA_Appv', 'DisbursementGross_times_SBA_Appv','DisbursementGross_divided_by_SBA_Appv']\n","sbagr = ['SBA_Appv_plus_GrAppv','SBA_Appv_minus_GrAppv', 'SBA_Appv_times_GrAppv','SBA_Appv_divided_by_GrAppv']\n","disgr = ['DisbursementGross_plus_GrAppv','DisbursementGross_minus_GrAppv', 'DisbursementGross_times_GrAppv','DisbursementGross_divided_by_GrAppv']\n","# year_col = [\"USREC_SUM\"]\n","# urb_col = [\"HOUPCT_RUR\",\"ALAND_PCT_RUR\",\"RUR_pct\"]\n","city_col = [\"city_Term_mean\",\"city_NoEmp_mean\"]\n","\n","cols_notcat = cols_notcat + city_col + dissba + sbagr + disgr + time_col # 数値特徴量\n","###-----------------------------------------------------------------------------\n","\n","col_cat = [c for c in train.columns if not c in cols_notcat] # カテゴリ特徴量\n","\n","# ターゲットエンコカラム\n","target_encoded_cols = [c for c in train.columns if \"target_\" in c]\n","# ordinalカテゴリ\n","ordinal_cols_cat = [c for c in col_cat if not c in target_encoded_cols]\n","\n","# カテゴリ特徴量のカラム\n","cols_cat = ordinal_cols_cat + target_encoded_cols\n","\n","for col in cols_cat:\n","    # もし欠損値があればその特徴量のユニーク数(欠損値を除く)で埋める\n","    num_null = train[col].nunique() - 1\n","    train[col].fillna(num_null, inplace=True)\n","    test[col].fillna(num_null, inplace=True)\n","\n","# 100以上のユニーク数を持つ特徴量を削除\n","cols_to_remove = [c for c in cols_exp if train[c].nunique() > 100]\n","cols_exp = [c for c in cols_exp if c not in cols_to_remove]\n","cols_cat = [c for c in cols_cat if c not in cols_to_remove]"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":466,"status":"ok","timestamp":1707922005006,"user":{"displayName":"田口采樹","userId":"02806376211526518224"},"user_tz":-540},"id":"r2v6We4F08rO","colab":{"base_uri":"https://localhost:8080/","height":443},"outputId":"3e718843-bbe0-4097-b06e-db2ebaec55e3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Term  NoEmp  NewExist  CreateJob  RetainedJob  FranchiseCode  \\\n","0       163     21       1.0          0            0              1   \n","1        84      6       1.0          4            0              0   \n","2       242     45       1.0          4           90              0   \n","3       237      4       1.0          0            0              0   \n","4       184      0       1.0          0            0              0   \n","...     ...    ...       ...        ...          ...            ...   \n","42302   283     14       1.0          0            0              1   \n","42303    53      2       1.0          0            0              0   \n","42304    59      6       2.0          0            0              1   \n","42305   295     18       1.0          0            8              0   \n","42306    84      4       1.0          0            8              0   \n","\n","       RevLineCr  LowDoc  MIS_Status  Sector  ...  \\\n","0              1       3           1      23  ...   \n","1              0       3           1      19  ...   \n","2              1       3           1       7  ...   \n","3              1       3           1       6  ...   \n","4              1       3           1      23  ...   \n","...          ...     ...         ...     ...  ...   \n","42302          1       3           1      23  ...   \n","42303          4       3           1       7  ...   \n","42304          1       3           1       7  ...   \n","42305          1       3           1       7  ...   \n","42306          1       3           1      21  ...   \n","\n","       DisbursementGross_plus_GrAppv  DisbursementGross_minus_GrAppv  \\\n","0                          -0.392578                        -0.09137   \n","1                           0.448242                        -0.09137   \n","2                          -0.591797                        -0.05780   \n","3                           0.212524                        -0.09137   \n","4                           1.415039                        -0.09137   \n","...                              ...                             ...   \n","42302                      -0.392578                        -0.09137   \n","42303                      -0.697266                        -0.09137   \n","42304                      -0.473877                        -0.09137   \n","42305                       0.476562                        -0.09137   \n","42306                      -0.443359                        -0.09137   \n","\n","       DisbursementGross_times_GrAppv  DisbursementGross_divided_by_GrAppv  \\\n","0                           -0.249634                            -0.248779   \n","1                           -0.025345                            -0.248779   \n","2                           -0.265625                            -0.129028   \n","3                           -0.113708                            -0.248779   \n","4                            0.544922                            -0.248779   \n","...                               ...                                  ...   \n","42302                       -0.249634                            -0.248779   \n","42303                       -0.268311                            -0.248779   \n","42304                       -0.257812                            -0.248779   \n","42305                       -0.013344                            -0.248779   \n","42306                       -0.255127                            -0.248779   \n","\n","       lr_avg_ppi   outlier  DisbursementDate_timestamp  \\\n","0        0.435791  0.090149                   -0.559570   \n","1       -0.380371  0.113342                   -1.242188   \n","2        0.137207  0.020538                    0.015060   \n","3       -0.122314  0.114258                    0.978027   \n","4       -0.623535  0.062927                   -2.912109   \n","...           ...       ...                         ...   \n","42302    0.009750  0.087708                   -0.559570   \n","42303   -0.444092  0.063965                   -1.656250   \n","42304    1.329102  0.105591                    0.254883   \n","42305    0.176514  0.024857                   -0.582520   \n","42306   -0.389160  0.084229                   -1.883789   \n","\n","       ApprovalDate_timestamp  LoanEnd_timestamp  ApprovalToDisbursement_days  \n","0                    0.846680           0.112244                     0.353271  \n","1                   -1.584961          -1.061523                     0.496338  \n","2                   -0.081238           1.213867                     0.482666  \n","3                    0.340332           1.818359                     0.533203  \n","4                   -0.309570          -1.285156                     0.240234  \n","...                       ...                ...                          ...  \n","42302               -1.128906           1.195312                     0.519043  \n","42303                0.966797          -1.620117                     0.245239  \n","42304                0.243896          -0.276611                     0.476807  \n","42305               -2.072266           1.289062                     0.596191  \n","42306                1.625000          -1.494141                     0.169678  \n","\n","[42307 rows x 41 columns]"],"text/html":["\n","  <div id=\"df-a3e57ad2-a3b5-40b2-8133-c239d62398df\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Term</th>\n","      <th>NoEmp</th>\n","      <th>NewExist</th>\n","      <th>CreateJob</th>\n","      <th>RetainedJob</th>\n","      <th>FranchiseCode</th>\n","      <th>RevLineCr</th>\n","      <th>LowDoc</th>\n","      <th>MIS_Status</th>\n","      <th>Sector</th>\n","      <th>...</th>\n","      <th>DisbursementGross_plus_GrAppv</th>\n","      <th>DisbursementGross_minus_GrAppv</th>\n","      <th>DisbursementGross_times_GrAppv</th>\n","      <th>DisbursementGross_divided_by_GrAppv</th>\n","      <th>lr_avg_ppi</th>\n","      <th>outlier</th>\n","      <th>DisbursementDate_timestamp</th>\n","      <th>ApprovalDate_timestamp</th>\n","      <th>LoanEnd_timestamp</th>\n","      <th>ApprovalToDisbursement_days</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>163</td>\n","      <td>21</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>...</td>\n","      <td>-0.392578</td>\n","      <td>-0.09137</td>\n","      <td>-0.249634</td>\n","      <td>-0.248779</td>\n","      <td>0.435791</td>\n","      <td>0.090149</td>\n","      <td>-0.559570</td>\n","      <td>0.846680</td>\n","      <td>0.112244</td>\n","      <td>0.353271</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>84</td>\n","      <td>6</td>\n","      <td>1.0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>19</td>\n","      <td>...</td>\n","      <td>0.448242</td>\n","      <td>-0.09137</td>\n","      <td>-0.025345</td>\n","      <td>-0.248779</td>\n","      <td>-0.380371</td>\n","      <td>0.113342</td>\n","      <td>-1.242188</td>\n","      <td>-1.584961</td>\n","      <td>-1.061523</td>\n","      <td>0.496338</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>242</td>\n","      <td>45</td>\n","      <td>1.0</td>\n","      <td>4</td>\n","      <td>90</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>-0.591797</td>\n","      <td>-0.05780</td>\n","      <td>-0.265625</td>\n","      <td>-0.129028</td>\n","      <td>0.137207</td>\n","      <td>0.020538</td>\n","      <td>0.015060</td>\n","      <td>-0.081238</td>\n","      <td>1.213867</td>\n","      <td>0.482666</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>237</td>\n","      <td>4</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>...</td>\n","      <td>0.212524</td>\n","      <td>-0.09137</td>\n","      <td>-0.113708</td>\n","      <td>-0.248779</td>\n","      <td>-0.122314</td>\n","      <td>0.114258</td>\n","      <td>0.978027</td>\n","      <td>0.340332</td>\n","      <td>1.818359</td>\n","      <td>0.533203</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>184</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>...</td>\n","      <td>1.415039</td>\n","      <td>-0.09137</td>\n","      <td>0.544922</td>\n","      <td>-0.248779</td>\n","      <td>-0.623535</td>\n","      <td>0.062927</td>\n","      <td>-2.912109</td>\n","      <td>-0.309570</td>\n","      <td>-1.285156</td>\n","      <td>0.240234</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>42302</th>\n","      <td>283</td>\n","      <td>14</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>...</td>\n","      <td>-0.392578</td>\n","      <td>-0.09137</td>\n","      <td>-0.249634</td>\n","      <td>-0.248779</td>\n","      <td>0.009750</td>\n","      <td>0.087708</td>\n","      <td>-0.559570</td>\n","      <td>-1.128906</td>\n","      <td>1.195312</td>\n","      <td>0.519043</td>\n","    </tr>\n","    <tr>\n","      <th>42303</th>\n","      <td>53</td>\n","      <td>2</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>-0.697266</td>\n","      <td>-0.09137</td>\n","      <td>-0.268311</td>\n","      <td>-0.248779</td>\n","      <td>-0.444092</td>\n","      <td>0.063965</td>\n","      <td>-1.656250</td>\n","      <td>0.966797</td>\n","      <td>-1.620117</td>\n","      <td>0.245239</td>\n","    </tr>\n","    <tr>\n","      <th>42304</th>\n","      <td>59</td>\n","      <td>6</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>-0.473877</td>\n","      <td>-0.09137</td>\n","      <td>-0.257812</td>\n","      <td>-0.248779</td>\n","      <td>1.329102</td>\n","      <td>0.105591</td>\n","      <td>0.254883</td>\n","      <td>0.243896</td>\n","      <td>-0.276611</td>\n","      <td>0.476807</td>\n","    </tr>\n","    <tr>\n","      <th>42305</th>\n","      <td>295</td>\n","      <td>18</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>0.476562</td>\n","      <td>-0.09137</td>\n","      <td>-0.013344</td>\n","      <td>-0.248779</td>\n","      <td>0.176514</td>\n","      <td>0.024857</td>\n","      <td>-0.582520</td>\n","      <td>-2.072266</td>\n","      <td>1.289062</td>\n","      <td>0.596191</td>\n","    </tr>\n","    <tr>\n","      <th>42306</th>\n","      <td>84</td>\n","      <td>4</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>21</td>\n","      <td>...</td>\n","      <td>-0.443359</td>\n","      <td>-0.09137</td>\n","      <td>-0.255127</td>\n","      <td>-0.248779</td>\n","      <td>-0.389160</td>\n","      <td>0.084229</td>\n","      <td>-1.883789</td>\n","      <td>1.625000</td>\n","      <td>-1.494141</td>\n","      <td>0.169678</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>42307 rows × 41 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3e57ad2-a3b5-40b2-8133-c239d62398df')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a3e57ad2-a3b5-40b2-8133-c239d62398df button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a3e57ad2-a3b5-40b2-8133-c239d62398df');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-dea35508-5b2f-4bf5-bbf6-c3616fbf737f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dea35508-5b2f-4bf5-bbf6-c3616fbf737f')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-dea35508-5b2f-4bf5-bbf6-c3616fbf737f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_1ba9f92a-99c9-4d9e-be9b-63ff372628f4\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_1ba9f92a-99c9-4d9e-be9b-63ff372628f4 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('train');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train"}},"metadata":{},"execution_count":7}],"source":["train"]},{"cell_type":"markdown","metadata":{"id":"N6Y8r8jQw_8Q"},"source":["**Tabnet**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"36KE5lNuxArf","executionInfo":{"status":"ok","timestamp":1707922005006,"user_tz":-540,"elapsed":3,"user":{"displayName":"田口采樹","userId":"02806376211526518224"}}},"outputs":[],"source":["def train_tabnet(train, cols_exp, cols_cat, col_target, params=None):\n","    if params is None:\n","        params = {}\n","\n","    params_add = {\"device_name\": \"cpu\", \"seed\": 0}\n","    params |= params_add\n","\n","    x = train[cols_exp].values\n","    y = train[col_target].values\n","\n","    # カテゴリ変数のインデックスと次元\n","    cols_cat_idxs = [cols_exp.index(c) for c in cols_cat]\n","    cols_num_idxs = [cols_exp.index(c) for c in cols_exp if c not in cols_cat]\n","    cols_cat_dims = [train[c].nunique() for c in cols_cat]\n","\n","    # K-fold\n","    kf = KFold(n_splits=4, shuffle=True, random_state=1)\n","    y_valid_pred_lst = []\n","    idx_valid_lst = []\n","    clf_lst = []\n","\n","    for fold, (idx_train, idx_valid) in enumerate(kf.split(x)):\n","        print(\"fold\", fold)\n","        x_train, x_valid = x[idx_train], x[idx_valid]\n","        y_train, y_valid = y[idx_train], y[idx_valid]\n","\n","        # 数値特徴量の正規化\n","        if cols_num_idxs:  # cols_num_idxsが空でない場合にのみ実行\n","            scaler = StandardScaler()\n","            # 数値特徴量にのみスケーラーを適用\n","            x_train[:, cols_num_idxs] = scaler.fit_transform(x_train[:, cols_num_idxs])\n","            x_valid[:, cols_num_idxs] = scaler.transform(x_valid[:, cols_num_idxs])\n","\n","        # モデリング\n","        pretrainer = TabNetPretrainer(**params)\n","        pretrainer.fit(x_train, eval_set=[x_valid])\n","        clf = TabNetClassifier(**params, cat_idxs=cols_cat_idxs, cat_dims=cols_cat_dims)\n","        clf.fit(\n","            x_train, y_train,\n","            eval_set=[(x_train, y_train), (x_valid, y_valid)],\n","            eval_name=['train', 'valid'],\n","            eval_metric=[\"logloss\"],\n","            from_unsupervised=pretrainer\n","        )\n","\n","        # OOF予測\n","        y_valid_pred = clf.predict_proba(x_valid)\n","        y_valid_pred_lst.append(y_valid_pred)\n","        idx_valid_lst.append(idx_valid)\n","        clf_lst.append(clf)\n","\n","    # OOF予測の統合\n","    idx_valid = np.hstack(idx_valid_lst)\n","    y_valid_pred = np.vstack(y_valid_pred_lst)\n","    oof_pred = y_valid_pred[np.argsort(idx_valid)]\n","\n","    return clf_lst, oof_pred"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"7gnaF_83xEXA","executionInfo":{"status":"ok","timestamp":1707922005641,"user_tz":-540,"elapsed":637,"user":{"displayName":"田口采樹","userId":"02806376211526518224"}}},"outputs":[],"source":["def predict_test(x_test, clf_lst):\n","    y_test_pred_lst = []\n","\n","    for clf in clf_lst:\n","        y_test_pred = clf.predict_proba(x_test)\n","        y_test_pred_lst.append(y_test_pred)\n","\n","    y_test_pred = np.mean(y_test_pred_lst, axis=0)\n","    return y_test_pred"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZbD4iVL2xGOq","executionInfo":{"status":"ok","timestamp":1707923194589,"user_tz":-540,"elapsed":1188951,"user":{"displayName":"田口采樹","userId":"02806376211526518224"}},"outputId":"b4954cc2-c836-4dde-861d-27b7a722f384"},"outputs":[{"output_type":"stream","name":"stdout","text":["fold 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 3.56977 | val_0_unsup_loss_numpy: 2.684619903564453|  0:00:04s\n","epoch 1  | loss: 1.37578 | val_0_unsup_loss_numpy: 1.1567000150680542|  0:00:10s\n","epoch 2  | loss: 1.01279 | val_0_unsup_loss_numpy: 1.075600028038025|  0:00:14s\n","epoch 3  | loss: 0.98907 | val_0_unsup_loss_numpy: 1.085919976234436|  0:00:16s\n","epoch 4  | loss: 0.98647 | val_0_unsup_loss_numpy: 1.0653899908065796|  0:00:18s\n","epoch 5  | loss: 0.97298 | val_0_unsup_loss_numpy: 1.0187499523162842|  0:00:21s\n","epoch 6  | loss: 0.969   | val_0_unsup_loss_numpy: 0.9766299724578857|  0:00:23s\n","epoch 7  | loss: 0.96391 | val_0_unsup_loss_numpy: 0.964900016784668|  0:00:25s\n","epoch 8  | loss: 0.96622 | val_0_unsup_loss_numpy: 0.9339100122451782|  0:00:27s\n","epoch 9  | loss: 0.96587 | val_0_unsup_loss_numpy: 0.9185799956321716|  0:00:29s\n","epoch 10 | loss: 0.96219 | val_0_unsup_loss_numpy: 0.8606299757957458|  0:00:31s\n","epoch 11 | loss: 0.94687 | val_0_unsup_loss_numpy: 0.8760700225830078|  0:00:34s\n","epoch 12 | loss: 0.94565 | val_0_unsup_loss_numpy: 0.8506799936294556|  0:00:36s\n","epoch 13 | loss: 0.94082 | val_0_unsup_loss_numpy: 0.8759099841117859|  0:00:38s\n","epoch 14 | loss: 0.94184 | val_0_unsup_loss_numpy: 0.8813899755477905|  0:00:40s\n","epoch 15 | loss: 0.9362  | val_0_unsup_loss_numpy: 0.8773800134658813|  0:00:42s\n","epoch 16 | loss: 0.94453 | val_0_unsup_loss_numpy: 0.852180004119873|  0:00:44s\n","epoch 17 | loss: 0.92928 | val_0_unsup_loss_numpy: 0.8560100197792053|  0:00:46s\n","epoch 18 | loss: 0.9289  | val_0_unsup_loss_numpy: 0.8519099950790405|  0:00:49s\n","epoch 19 | loss: 0.92379 | val_0_unsup_loss_numpy: 0.8428999781608582|  0:00:51s\n","epoch 20 | loss: 0.92243 | val_0_unsup_loss_numpy: 0.834909975528717|  0:00:53s\n","epoch 21 | loss: 0.92613 | val_0_unsup_loss_numpy: 0.8260899782180786|  0:00:54s\n","epoch 22 | loss: 0.92931 | val_0_unsup_loss_numpy: 0.8024799823760986|  0:00:56s\n","epoch 23 | loss: 0.91998 | val_0_unsup_loss_numpy: 0.8218500018119812|  0:00:58s\n","epoch 24 | loss: 0.91508 | val_0_unsup_loss_numpy: 0.8071799874305725|  0:01:01s\n","epoch 25 | loss: 0.92219 | val_0_unsup_loss_numpy: 0.7835299968719482|  0:01:03s\n","epoch 26 | loss: 0.91327 | val_0_unsup_loss_numpy: 0.7681000232696533|  0:01:05s\n","epoch 27 | loss: 0.91106 | val_0_unsup_loss_numpy: 0.7559700012207031|  0:01:07s\n","epoch 28 | loss: 0.90293 | val_0_unsup_loss_numpy: 0.7568100094795227|  0:01:09s\n","epoch 29 | loss: 0.90866 | val_0_unsup_loss_numpy: 0.7516599893569946|  0:01:11s\n","epoch 30 | loss: 0.89475 | val_0_unsup_loss_numpy: 0.7570599913597107|  0:01:13s\n","epoch 31 | loss: 0.90393 | val_0_unsup_loss_numpy: 0.7401700019836426|  0:01:16s\n","epoch 32 | loss: 0.8861  | val_0_unsup_loss_numpy: 0.7246699929237366|  0:01:18s\n","epoch 33 | loss: 0.89966 | val_0_unsup_loss_numpy: 0.7332299947738647|  0:01:21s\n","epoch 34 | loss: 0.89367 | val_0_unsup_loss_numpy: 0.7230700254440308|  0:01:23s\n","epoch 35 | loss: 0.89233 | val_0_unsup_loss_numpy: 0.7273300290107727|  0:01:25s\n","epoch 36 | loss: 0.8937  | val_0_unsup_loss_numpy: 0.7308700084686279|  0:01:27s\n","epoch 37 | loss: 0.88748 | val_0_unsup_loss_numpy: 0.7197499871253967|  0:01:30s\n","epoch 38 | loss: 0.88742 | val_0_unsup_loss_numpy: 0.7159500122070312|  0:01:31s\n","epoch 39 | loss: 0.88923 | val_0_unsup_loss_numpy: 0.7119399905204773|  0:01:33s\n","epoch 40 | loss: 0.88768 | val_0_unsup_loss_numpy: 0.707539975643158|  0:01:35s\n","epoch 41 | loss: 0.88196 | val_0_unsup_loss_numpy: 0.7070599794387817|  0:01:37s\n","epoch 42 | loss: 0.87469 | val_0_unsup_loss_numpy: 0.6988000273704529|  0:01:39s\n","epoch 43 | loss: 0.88042 | val_0_unsup_loss_numpy: 0.6992800235748291|  0:01:42s\n","epoch 44 | loss: 0.87831 | val_0_unsup_loss_numpy: 0.7155900001525879|  0:01:44s\n","epoch 45 | loss: 0.87248 | val_0_unsup_loss_numpy: 0.6926800012588501|  0:01:46s\n","epoch 46 | loss: 0.88793 | val_0_unsup_loss_numpy: 0.7278599739074707|  0:01:48s\n","epoch 47 | loss: 0.89389 | val_0_unsup_loss_numpy: 0.70100998878479|  0:01:50s\n","epoch 48 | loss: 0.88418 | val_0_unsup_loss_numpy: 0.7021200060844421|  0:01:52s\n","epoch 49 | loss: 0.87874 | val_0_unsup_loss_numpy: 0.7085000276565552|  0:01:55s\n","epoch 50 | loss: 0.87905 | val_0_unsup_loss_numpy: 0.7132800221443176|  0:01:57s\n","epoch 51 | loss: 0.88763 | val_0_unsup_loss_numpy: 0.6974800229072571|  0:01:59s\n","epoch 52 | loss: 0.88432 | val_0_unsup_loss_numpy: 0.714739978313446|  0:02:01s\n","epoch 53 | loss: 0.87806 | val_0_unsup_loss_numpy: 0.7134600281715393|  0:02:03s\n","epoch 54 | loss: 0.88157 | val_0_unsup_loss_numpy: 0.702530026435852|  0:02:05s\n","epoch 55 | loss: 0.89017 | val_0_unsup_loss_numpy: 0.7211499810218811|  0:02:08s\n","\n","Early stopping occurred at epoch 55 with best_epoch = 45 and best_val_0_unsup_loss_numpy = 0.6926800012588501\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [5, 7, 24, 38, 51, 52, 3] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [3, 4, 5, 6, 7, 8, 9] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n","  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.45892 | train_logloss: 0.35555 | valid_logloss: 0.35777 |  0:00:02s\n","epoch 1  | loss: 0.3119  | train_logloss: 0.40839 | valid_logloss: 0.40723 |  0:00:05s\n","epoch 2  | loss: 0.30676 | train_logloss: 0.40498 | valid_logloss: 0.40285 |  0:00:07s\n","epoch 3  | loss: 0.30502 | train_logloss: 0.33037 | valid_logloss: 0.33233 |  0:00:09s\n","epoch 4  | loss: 0.30352 | train_logloss: 0.30812 | valid_logloss: 0.31104 |  0:00:13s\n","epoch 5  | loss: 0.30021 | train_logloss: 0.30413 | valid_logloss: 0.30838 |  0:00:15s\n","epoch 6  | loss: 0.3014  | train_logloss: 0.30252 | valid_logloss: 0.30945 |  0:00:18s\n","epoch 7  | loss: 0.30192 | train_logloss: 0.29684 | valid_logloss: 0.30416 |  0:00:20s\n","epoch 8  | loss: 0.29828 | train_logloss: 0.29689 | valid_logloss: 0.3016  |  0:00:22s\n","epoch 9  | loss: 0.29978 | train_logloss: 0.29628 | valid_logloss: 0.30145 |  0:00:26s\n","epoch 10 | loss: 0.29681 | train_logloss: 0.29599 | valid_logloss: 0.30191 |  0:00:28s\n","epoch 11 | loss: 0.29713 | train_logloss: 0.29495 | valid_logloss: 0.29971 |  0:00:31s\n","epoch 12 | loss: 0.29628 | train_logloss: 0.29377 | valid_logloss: 0.29881 |  0:00:33s\n","epoch 13 | loss: 0.29558 | train_logloss: 0.29392 | valid_logloss: 0.29965 |  0:00:35s\n","epoch 14 | loss: 0.29566 | train_logloss: 0.29194 | valid_logloss: 0.29755 |  0:00:38s\n","epoch 15 | loss: 0.29637 | train_logloss: 0.29203 | valid_logloss: 0.29802 |  0:00:41s\n","epoch 16 | loss: 0.29334 | train_logloss: 0.29116 | valid_logloss: 0.2973  |  0:00:43s\n","epoch 17 | loss: 0.29408 | train_logloss: 0.29191 | valid_logloss: 0.29939 |  0:00:46s\n","epoch 18 | loss: 0.29385 | train_logloss: 0.29176 | valid_logloss: 0.29776 |  0:00:48s\n","epoch 19 | loss: 0.29599 | train_logloss: 0.29284 | valid_logloss: 0.29731 |  0:00:51s\n","epoch 20 | loss: 0.29643 | train_logloss: 0.29436 | valid_logloss: 0.29983 |  0:00:54s\n","epoch 21 | loss: 0.29453 | train_logloss: 0.29057 | valid_logloss: 0.29538 |  0:00:56s\n","epoch 22 | loss: 0.29395 | train_logloss: 0.29104 | valid_logloss: 0.29619 |  0:00:58s\n","epoch 23 | loss: 0.29302 | train_logloss: 0.29103 | valid_logloss: 0.29707 |  0:01:01s\n","epoch 24 | loss: 0.29199 | train_logloss: 0.29154 | valid_logloss: 0.29943 |  0:01:04s\n","epoch 25 | loss: 0.29247 | train_logloss: 0.29164 | valid_logloss: 0.29899 |  0:01:07s\n","epoch 26 | loss: 0.29297 | train_logloss: 0.28984 | valid_logloss: 0.2966  |  0:01:09s\n","epoch 27 | loss: 0.29076 | train_logloss: 0.28924 | valid_logloss: 0.29568 |  0:01:12s\n","epoch 28 | loss: 0.29154 | train_logloss: 0.28953 | valid_logloss: 0.29656 |  0:01:14s\n","epoch 29 | loss: 0.29108 | train_logloss: 0.28948 | valid_logloss: 0.29769 |  0:01:17s\n","epoch 30 | loss: 0.29204 | train_logloss: 0.28909 | valid_logloss: 0.29636 |  0:01:20s\n","epoch 31 | loss: 0.2917  | train_logloss: 0.28954 | valid_logloss: 0.29642 |  0:01:23s\n","\n","Early stopping occurred at epoch 31 with best_epoch = 21 and best_valid_logloss = 0.29538\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"output_type":"stream","name":"stdout","text":["fold 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 3.61009 | val_0_unsup_loss_numpy: 2.530669927597046|  0:00:01s\n","epoch 1  | loss: 1.37589 | val_0_unsup_loss_numpy: 1.1400099992752075|  0:00:03s\n","epoch 2  | loss: 1.00406 | val_0_unsup_loss_numpy: 1.0423500537872314|  0:00:06s\n","epoch 3  | loss: 0.98373 | val_0_unsup_loss_numpy: 1.0475800037384033|  0:00:08s\n","epoch 4  | loss: 0.98176 | val_0_unsup_loss_numpy: 0.9504600167274475|  0:00:10s\n","epoch 5  | loss: 0.97126 | val_0_unsup_loss_numpy: 0.9257500171661377|  0:00:12s\n","epoch 6  | loss: 0.97102 | val_0_unsup_loss_numpy: 0.916700005531311|  0:00:14s\n","epoch 7  | loss: 0.9642  | val_0_unsup_loss_numpy: 0.9161900281906128|  0:00:16s\n","epoch 8  | loss: 0.96918 | val_0_unsup_loss_numpy: 0.9282600283622742|  0:00:18s\n","epoch 9  | loss: 0.9551  | val_0_unsup_loss_numpy: 0.9162300229072571|  0:00:21s\n","epoch 10 | loss: 0.94204 | val_0_unsup_loss_numpy: 0.88823002576828|  0:00:24s\n","epoch 11 | loss: 0.94858 | val_0_unsup_loss_numpy: 0.873960018157959|  0:00:26s\n","epoch 12 | loss: 0.95065 | val_0_unsup_loss_numpy: 0.8874199986457825|  0:00:28s\n","epoch 13 | loss: 0.95217 | val_0_unsup_loss_numpy: 0.8809900283813477|  0:00:30s\n","epoch 14 | loss: 0.93633 | val_0_unsup_loss_numpy: 0.8770400285720825|  0:00:32s\n","epoch 15 | loss: 0.93843 | val_0_unsup_loss_numpy: 0.865559995174408|  0:00:35s\n","epoch 16 | loss: 0.94534 | val_0_unsup_loss_numpy: 0.8605300188064575|  0:00:37s\n","epoch 17 | loss: 0.94604 | val_0_unsup_loss_numpy: 0.8557800054550171|  0:00:39s\n","epoch 18 | loss: 0.9335  | val_0_unsup_loss_numpy: 0.8429999947547913|  0:00:41s\n","epoch 19 | loss: 0.9213  | val_0_unsup_loss_numpy: 0.8329899907112122|  0:00:43s\n","epoch 20 | loss: 0.91925 | val_0_unsup_loss_numpy: 0.8278399705886841|  0:00:45s\n","epoch 21 | loss: 0.92431 | val_0_unsup_loss_numpy: 0.8216999769210815|  0:00:48s\n","epoch 22 | loss: 0.92928 | val_0_unsup_loss_numpy: 0.8235099911689758|  0:00:50s\n","epoch 23 | loss: 0.91882 | val_0_unsup_loss_numpy: 0.8181599974632263|  0:00:52s\n","epoch 24 | loss: 0.92287 | val_0_unsup_loss_numpy: 0.7919899821281433|  0:00:54s\n","epoch 25 | loss: 0.91934 | val_0_unsup_loss_numpy: 0.7844499945640564|  0:00:56s\n","epoch 26 | loss: 0.91653 | val_0_unsup_loss_numpy: 0.7991799712181091|  0:00:58s\n","epoch 27 | loss: 0.92716 | val_0_unsup_loss_numpy: 0.7954400181770325|  0:01:00s\n","epoch 28 | loss: 0.91854 | val_0_unsup_loss_numpy: 0.8062999844551086|  0:01:03s\n","epoch 29 | loss: 0.91096 | val_0_unsup_loss_numpy: 0.7992100119590759|  0:01:04s\n","epoch 30 | loss: 0.90976 | val_0_unsup_loss_numpy: 0.7965800166130066|  0:01:06s\n","epoch 31 | loss: 0.9145  | val_0_unsup_loss_numpy: 0.8057199716567993|  0:01:08s\n","epoch 32 | loss: 0.90422 | val_0_unsup_loss_numpy: 0.803629994392395|  0:01:10s\n","epoch 33 | loss: 0.90699 | val_0_unsup_loss_numpy: 0.7927200198173523|  0:01:13s\n","epoch 34 | loss: 0.91845 | val_0_unsup_loss_numpy: 0.7972300052642822|  0:01:16s\n","epoch 35 | loss: 0.92597 | val_0_unsup_loss_numpy: 0.8015699982643127|  0:01:18s\n","\n","Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_unsup_loss_numpy = 0.7844499945640564\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [5, 7, 24, 38, 51, 52, 3] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [3, 4, 5, 6, 7, 8, 9] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n","  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.57615 | train_logloss: 0.38625 | valid_logloss: 0.38662 |  0:00:02s\n","epoch 1  | loss: 0.31539 | train_logloss: 0.35886 | valid_logloss: 0.35452 |  0:00:04s\n","epoch 2  | loss: 0.31109 | train_logloss: 0.3503  | valid_logloss: 0.34639 |  0:00:06s\n","epoch 3  | loss: 0.3108  | train_logloss: 0.31826 | valid_logloss: 0.31508 |  0:00:10s\n","epoch 4  | loss: 0.3074  | train_logloss: 0.31559 | valid_logloss: 0.31241 |  0:00:12s\n","epoch 5  | loss: 0.31039 | train_logloss: 0.31322 | valid_logloss: 0.31011 |  0:00:15s\n","epoch 6  | loss: 0.30975 | train_logloss: 0.30895 | valid_logloss: 0.30649 |  0:00:17s\n","epoch 7  | loss: 0.30932 | train_logloss: 0.30698 | valid_logloss: 0.30448 |  0:00:20s\n","epoch 8  | loss: 0.30866 | train_logloss: 0.31023 | valid_logloss: 0.30626 |  0:00:23s\n","epoch 9  | loss: 0.30878 | train_logloss: 0.30746 | valid_logloss: 0.30455 |  0:00:26s\n","epoch 10 | loss: 0.30746 | train_logloss: 0.30728 | valid_logloss: 0.30454 |  0:00:28s\n","epoch 11 | loss: 0.30808 | train_logloss: 0.30584 | valid_logloss: 0.30322 |  0:00:31s\n","epoch 12 | loss: 0.30632 | train_logloss: 0.30574 | valid_logloss: 0.3026  |  0:00:33s\n","epoch 13 | loss: 0.3069  | train_logloss: 0.30499 | valid_logloss: 0.30177 |  0:00:36s\n","epoch 14 | loss: 0.30641 | train_logloss: 0.30428 | valid_logloss: 0.30174 |  0:00:39s\n","epoch 15 | loss: 0.30498 | train_logloss: 0.31482 | valid_logloss: 0.31229 |  0:00:41s\n","epoch 16 | loss: 0.30777 | train_logloss: 0.3041  | valid_logloss: 0.30096 |  0:00:43s\n","epoch 17 | loss: 0.30519 | train_logloss: 0.30393 | valid_logloss: 0.3009  |  0:00:46s\n","epoch 18 | loss: 0.30518 | train_logloss: 0.30417 | valid_logloss: 0.30187 |  0:00:49s\n","epoch 19 | loss: 0.3046  | train_logloss: 0.30411 | valid_logloss: 0.30131 |  0:00:52s\n","epoch 20 | loss: 0.30556 | train_logloss: 0.30277 | valid_logloss: 0.29996 |  0:00:54s\n","epoch 21 | loss: 0.30593 | train_logloss: 0.3035  | valid_logloss: 0.30075 |  0:00:57s\n","epoch 22 | loss: 0.3047  | train_logloss: 0.3028  | valid_logloss: 0.30001 |  0:00:59s\n","epoch 23 | loss: 0.30378 | train_logloss: 0.30141 | valid_logloss: 0.2975  |  0:01:03s\n","epoch 24 | loss: 0.30296 | train_logloss: 0.3029  | valid_logloss: 0.30025 |  0:01:05s\n","epoch 25 | loss: 0.30388 | train_logloss: 0.30107 | valid_logloss: 0.29975 |  0:01:08s\n","epoch 26 | loss: 0.30386 | train_logloss: 0.30008 | valid_logloss: 0.2971  |  0:01:10s\n","epoch 27 | loss: 0.30333 | train_logloss: 0.30336 | valid_logloss: 0.30109 |  0:01:13s\n","epoch 28 | loss: 0.30304 | train_logloss: 0.30114 | valid_logloss: 0.29858 |  0:01:16s\n","epoch 29 | loss: 0.30217 | train_logloss: 0.30075 | valid_logloss: 0.29943 |  0:01:18s\n","epoch 30 | loss: 0.30199 | train_logloss: 0.30123 | valid_logloss: 0.29839 |  0:01:21s\n","epoch 31 | loss: 0.30148 | train_logloss: 0.29998 | valid_logloss: 0.29676 |  0:01:24s\n","epoch 32 | loss: 0.3013  | train_logloss: 0.30013 | valid_logloss: 0.29651 |  0:01:26s\n","epoch 33 | loss: 0.30154 | train_logloss: 0.30127 | valid_logloss: 0.2964  |  0:01:30s\n","epoch 34 | loss: 0.30254 | train_logloss: 0.30334 | valid_logloss: 0.3001  |  0:01:32s\n","epoch 35 | loss: 0.30379 | train_logloss: 0.29969 | valid_logloss: 0.2957  |  0:01:35s\n","epoch 36 | loss: 0.30171 | train_logloss: 0.29792 | valid_logloss: 0.29384 |  0:01:37s\n","epoch 37 | loss: 0.29977 | train_logloss: 0.29858 | valid_logloss: 0.29399 |  0:01:40s\n","epoch 38 | loss: 0.30063 | train_logloss: 0.30059 | valid_logloss: 0.29783 |  0:01:44s\n","epoch 39 | loss: 0.30075 | train_logloss: 0.29857 | valid_logloss: 0.29509 |  0:01:46s\n","epoch 40 | loss: 0.30015 | train_logloss: 0.2986  | valid_logloss: 0.2955  |  0:01:48s\n","epoch 41 | loss: 0.29764 | train_logloss: 0.29884 | valid_logloss: 0.29432 |  0:01:51s\n","epoch 42 | loss: 0.2987  | train_logloss: 0.2972  | valid_logloss: 0.29376 |  0:01:54s\n","epoch 43 | loss: 0.2993  | train_logloss: 0.29709 | valid_logloss: 0.29263 |  0:01:57s\n","epoch 44 | loss: 0.30099 | train_logloss: 0.29708 | valid_logloss: 0.29296 |  0:01:59s\n","epoch 45 | loss: 0.30005 | train_logloss: 0.29734 | valid_logloss: 0.29345 |  0:02:02s\n","epoch 46 | loss: 0.29894 | train_logloss: 0.2974  | valid_logloss: 0.29402 |  0:02:04s\n","epoch 47 | loss: 0.29981 | train_logloss: 0.29639 | valid_logloss: 0.29208 |  0:02:07s\n","epoch 48 | loss: 0.29767 | train_logloss: 0.29686 | valid_logloss: 0.29288 |  0:02:10s\n","epoch 49 | loss: 0.29802 | train_logloss: 0.29628 | valid_logloss: 0.29214 |  0:02:12s\n","epoch 50 | loss: 0.29818 | train_logloss: 0.29779 | valid_logloss: 0.29438 |  0:02:15s\n","epoch 51 | loss: 0.29867 | train_logloss: 0.29698 | valid_logloss: 0.29216 |  0:02:17s\n","epoch 52 | loss: 0.29757 | train_logloss: 0.29773 | valid_logloss: 0.29273 |  0:02:21s\n","epoch 53 | loss: 0.29868 | train_logloss: 0.29672 | valid_logloss: 0.29168 |  0:02:24s\n","epoch 54 | loss: 0.29863 | train_logloss: 0.29815 | valid_logloss: 0.293   |  0:02:26s\n","epoch 55 | loss: 0.29827 | train_logloss: 0.2966  | valid_logloss: 0.29163 |  0:02:28s\n","epoch 56 | loss: 0.29779 | train_logloss: 0.2977  | valid_logloss: 0.29297 |  0:02:31s\n","epoch 57 | loss: 0.29829 | train_logloss: 0.29587 | valid_logloss: 0.29042 |  0:02:33s\n","epoch 58 | loss: 0.2978  | train_logloss: 0.29766 | valid_logloss: 0.29223 |  0:02:36s\n","epoch 59 | loss: 0.29968 | train_logloss: 0.29677 | valid_logloss: 0.29127 |  0:02:39s\n","epoch 60 | loss: 0.29688 | train_logloss: 0.29739 | valid_logloss: 0.29279 |  0:02:41s\n","epoch 61 | loss: 0.29815 | train_logloss: 0.29685 | valid_logloss: 0.29222 |  0:02:44s\n","epoch 62 | loss: 0.2987  | train_logloss: 0.29653 | valid_logloss: 0.29162 |  0:02:46s\n","epoch 63 | loss: 0.29818 | train_logloss: 0.29605 | valid_logloss: 0.29079 |  0:02:50s\n","epoch 64 | loss: 0.29667 | train_logloss: 0.29549 | valid_logloss: 0.29034 |  0:02:52s\n","epoch 65 | loss: 0.2959  | train_logloss: 0.29499 | valid_logloss: 0.28976 |  0:02:54s\n","epoch 66 | loss: 0.29608 | train_logloss: 0.29479 | valid_logloss: 0.28936 |  0:02:57s\n","epoch 67 | loss: 0.29752 | train_logloss: 0.30261 | valid_logloss: 0.29783 |  0:02:59s\n","epoch 68 | loss: 0.29981 | train_logloss: 0.29531 | valid_logloss: 0.28969 |  0:03:03s\n","epoch 69 | loss: 0.29791 | train_logloss: 0.29626 | valid_logloss: 0.29021 |  0:03:05s\n","epoch 70 | loss: 0.29823 | train_logloss: 0.29544 | valid_logloss: 0.28957 |  0:03:07s\n","epoch 71 | loss: 0.29687 | train_logloss: 0.29578 | valid_logloss: 0.29062 |  0:03:10s\n","epoch 72 | loss: 0.29711 | train_logloss: 0.29486 | valid_logloss: 0.28908 |  0:03:12s\n","epoch 73 | loss: 0.29668 | train_logloss: 0.2956  | valid_logloss: 0.29033 |  0:03:16s\n","epoch 74 | loss: 0.2979  | train_logloss: 0.29824 | valid_logloss: 0.29252 |  0:03:19s\n","epoch 75 | loss: 0.29917 | train_logloss: 0.29597 | valid_logloss: 0.29117 |  0:03:22s\n","epoch 76 | loss: 0.29637 | train_logloss: 0.29539 | valid_logloss: 0.29097 |  0:03:24s\n","epoch 77 | loss: 0.29612 | train_logloss: 0.29594 | valid_logloss: 0.2913  |  0:03:27s\n","epoch 78 | loss: 0.29813 | train_logloss: 0.29552 | valid_logloss: 0.29034 |  0:03:30s\n","epoch 79 | loss: 0.29746 | train_logloss: 0.29482 | valid_logloss: 0.28971 |  0:03:32s\n","epoch 80 | loss: 0.29677 | train_logloss: 0.2955  | valid_logloss: 0.29061 |  0:03:35s\n","epoch 81 | loss: 0.29724 | train_logloss: 0.29573 | valid_logloss: 0.29049 |  0:03:37s\n","epoch 82 | loss: 0.29588 | train_logloss: 0.29468 | valid_logloss: 0.29001 |  0:03:39s\n","\n","Early stopping occurred at epoch 82 with best_epoch = 72 and best_valid_logloss = 0.28908\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"output_type":"stream","name":"stdout","text":["fold 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 3.60217 | val_0_unsup_loss_numpy: 3.1635799407958984|  0:00:02s\n","epoch 1  | loss: 1.41778 | val_0_unsup_loss_numpy: 1.1629999876022339|  0:00:04s\n","epoch 2  | loss: 0.99297 | val_0_unsup_loss_numpy: 1.12322998046875|  0:00:06s\n","epoch 3  | loss: 0.98514 | val_0_unsup_loss_numpy: 1.0354700088500977|  0:00:08s\n","epoch 4  | loss: 0.97731 | val_0_unsup_loss_numpy: 0.9934899806976318|  0:00:10s\n","epoch 5  | loss: 0.9703  | val_0_unsup_loss_numpy: 0.9636899828910828|  0:00:12s\n","epoch 6  | loss: 0.97213 | val_0_unsup_loss_numpy: 0.9637699723243713|  0:00:14s\n","epoch 7  | loss: 0.96525 | val_0_unsup_loss_numpy: 0.9661800265312195|  0:00:16s\n","epoch 8  | loss: 0.94977 | val_0_unsup_loss_numpy: 0.9531700015068054|  0:00:19s\n","epoch 9  | loss: 0.96007 | val_0_unsup_loss_numpy: 0.9516699910163879|  0:00:21s\n","epoch 10 | loss: 0.96238 | val_0_unsup_loss_numpy: 0.9407200217247009|  0:00:22s\n","epoch 11 | loss: 0.95637 | val_0_unsup_loss_numpy: 0.9308199882507324|  0:00:24s\n","epoch 12 | loss: 0.95133 | val_0_unsup_loss_numpy: 0.9110400080680847|  0:00:27s\n","epoch 13 | loss: 0.94835 | val_0_unsup_loss_numpy: 0.897379994392395|  0:00:29s\n","epoch 14 | loss: 0.93891 | val_0_unsup_loss_numpy: 0.8972499966621399|  0:00:31s\n","epoch 15 | loss: 0.94495 | val_0_unsup_loss_numpy: 0.8784899711608887|  0:00:33s\n","epoch 16 | loss: 0.93706 | val_0_unsup_loss_numpy: 0.8797900080680847|  0:00:35s\n","epoch 17 | loss: 0.92196 | val_0_unsup_loss_numpy: 0.8480299711227417|  0:00:37s\n","epoch 18 | loss: 0.93484 | val_0_unsup_loss_numpy: 0.8598799705505371|  0:00:40s\n","epoch 19 | loss: 0.92188 | val_0_unsup_loss_numpy: 0.8614599704742432|  0:00:42s\n","epoch 20 | loss: 0.92448 | val_0_unsup_loss_numpy: 0.8438900113105774|  0:00:44s\n","epoch 21 | loss: 0.91602 | val_0_unsup_loss_numpy: 0.8283399939537048|  0:00:46s\n","epoch 22 | loss: 0.90923 | val_0_unsup_loss_numpy: 0.8365300297737122|  0:00:48s\n","epoch 23 | loss: 0.90452 | val_0_unsup_loss_numpy: 0.8117300271987915|  0:00:50s\n","epoch 24 | loss: 0.91152 | val_0_unsup_loss_numpy: 0.8198999762535095|  0:00:53s\n","epoch 25 | loss: 0.91702 | val_0_unsup_loss_numpy: 0.8287299871444702|  0:00:55s\n","epoch 26 | loss: 0.90077 | val_0_unsup_loss_numpy: 0.8263400197029114|  0:00:57s\n","epoch 27 | loss: 0.91432 | val_0_unsup_loss_numpy: 0.8148599863052368|  0:00:59s\n","epoch 28 | loss: 0.89897 | val_0_unsup_loss_numpy: 0.7984200119972229|  0:01:01s\n","epoch 29 | loss: 0.896   | val_0_unsup_loss_numpy: 0.7916399836540222|  0:01:03s\n","epoch 30 | loss: 0.89867 | val_0_unsup_loss_numpy: 0.7911700010299683|  0:01:05s\n","epoch 31 | loss: 0.90524 | val_0_unsup_loss_numpy: 0.783079981803894|  0:01:08s\n","epoch 32 | loss: 0.89638 | val_0_unsup_loss_numpy: 0.7815799713134766|  0:01:10s\n","epoch 33 | loss: 0.91087 | val_0_unsup_loss_numpy: 0.771369993686676|  0:01:13s\n","epoch 34 | loss: 0.89676 | val_0_unsup_loss_numpy: 0.7886499762535095|  0:01:14s\n","epoch 35 | loss: 0.91116 | val_0_unsup_loss_numpy: 0.7757499814033508|  0:01:17s\n","epoch 36 | loss: 0.89828 | val_0_unsup_loss_numpy: 0.7866299748420715|  0:01:19s\n","epoch 37 | loss: 0.90101 | val_0_unsup_loss_numpy: 0.7690200209617615|  0:01:22s\n","epoch 38 | loss: 0.89606 | val_0_unsup_loss_numpy: 0.7601500153541565|  0:01:24s\n","epoch 39 | loss: 0.89749 | val_0_unsup_loss_numpy: 0.7478500008583069|  0:01:27s\n","epoch 40 | loss: 0.90269 | val_0_unsup_loss_numpy: 0.7880200147628784|  0:01:29s\n","epoch 41 | loss: 0.89159 | val_0_unsup_loss_numpy: 0.7778499722480774|  0:01:31s\n","epoch 42 | loss: 0.89113 | val_0_unsup_loss_numpy: 0.7938100099563599|  0:01:34s\n","epoch 43 | loss: 0.89629 | val_0_unsup_loss_numpy: 0.7640299797058105|  0:01:37s\n","epoch 44 | loss: 0.89893 | val_0_unsup_loss_numpy: 0.7613199949264526|  0:01:39s\n","epoch 45 | loss: 0.89342 | val_0_unsup_loss_numpy: 0.7634599804878235|  0:01:41s\n","epoch 46 | loss: 0.89271 | val_0_unsup_loss_numpy: 0.7336099743843079|  0:01:43s\n","epoch 47 | loss: 0.89171 | val_0_unsup_loss_numpy: 0.7559999823570251|  0:01:46s\n","epoch 48 | loss: 0.88211 | val_0_unsup_loss_numpy: 0.7372400164604187|  0:01:48s\n","epoch 49 | loss: 0.89406 | val_0_unsup_loss_numpy: 0.735480010509491|  0:01:51s\n","epoch 50 | loss: 0.897   | val_0_unsup_loss_numpy: 0.7383900284767151|  0:01:53s\n","epoch 51 | loss: 0.88154 | val_0_unsup_loss_numpy: 0.7512199878692627|  0:01:55s\n","epoch 52 | loss: 0.89082 | val_0_unsup_loss_numpy: 0.7350299954414368|  0:01:57s\n","epoch 53 | loss: 0.88569 | val_0_unsup_loss_numpy: 0.7496500015258789|  0:02:00s\n","epoch 54 | loss: 0.87851 | val_0_unsup_loss_numpy: 0.7367799878120422|  0:02:02s\n","epoch 55 | loss: 0.87475 | val_0_unsup_loss_numpy: 0.7554399967193604|  0:02:05s\n","epoch 56 | loss: 0.88972 | val_0_unsup_loss_numpy: 0.7199100255966187|  0:02:07s\n","epoch 57 | loss: 0.88643 | val_0_unsup_loss_numpy: 0.7350000143051147|  0:02:09s\n","epoch 58 | loss: 0.88318 | val_0_unsup_loss_numpy: 0.720579981803894|  0:02:11s\n","epoch 59 | loss: 0.89215 | val_0_unsup_loss_numpy: 0.7199299931526184|  0:02:13s\n","epoch 60 | loss: 0.88195 | val_0_unsup_loss_numpy: 0.7201899886131287|  0:02:16s\n","epoch 61 | loss: 0.88983 | val_0_unsup_loss_numpy: 0.7148500084877014|  0:02:17s\n","epoch 62 | loss: 0.87992 | val_0_unsup_loss_numpy: 0.7319300174713135|  0:02:20s\n","epoch 63 | loss: 0.88258 | val_0_unsup_loss_numpy: 0.7171900272369385|  0:02:22s\n","epoch 64 | loss: 0.8857  | val_0_unsup_loss_numpy: 0.7196800112724304|  0:02:24s\n","epoch 65 | loss: 0.88046 | val_0_unsup_loss_numpy: 0.701990008354187|  0:02:26s\n","epoch 66 | loss: 0.87797 | val_0_unsup_loss_numpy: 0.725409984588623|  0:02:29s\n","epoch 67 | loss: 0.88015 | val_0_unsup_loss_numpy: 0.7354599833488464|  0:02:31s\n","epoch 68 | loss: 0.88502 | val_0_unsup_loss_numpy: 0.7262600064277649|  0:02:33s\n","epoch 69 | loss: 0.88246 | val_0_unsup_loss_numpy: 0.7137600183486938|  0:02:35s\n","epoch 70 | loss: 0.88391 | val_0_unsup_loss_numpy: 0.7308899760246277|  0:02:37s\n","epoch 71 | loss: 0.88494 | val_0_unsup_loss_numpy: 0.7213000059127808|  0:02:40s\n","epoch 72 | loss: 0.86943 | val_0_unsup_loss_numpy: 0.7200999855995178|  0:02:42s\n","epoch 73 | loss: 0.88875 | val_0_unsup_loss_numpy: 0.7192999720573425|  0:02:44s\n","epoch 74 | loss: 0.88727 | val_0_unsup_loss_numpy: 0.7106099724769592|  0:02:46s\n","epoch 75 | loss: 0.8843  | val_0_unsup_loss_numpy: 0.715399980545044|  0:02:48s\n","\n","Early stopping occurred at epoch 75 with best_epoch = 65 and best_val_0_unsup_loss_numpy = 0.701990008354187\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [5, 7, 24, 38, 51, 52, 3] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [3, 4, 5, 6, 7, 8, 9] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n","  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.63009 | train_logloss: 0.35063 | valid_logloss: 0.35414 |  0:00:02s\n","epoch 1  | loss: 0.31294 | train_logloss: 0.36332 | valid_logloss: 0.36855 |  0:00:06s\n","epoch 2  | loss: 0.30519 | train_logloss: 0.3861  | valid_logloss: 0.39029 |  0:00:08s\n","epoch 3  | loss: 0.30241 | train_logloss: 0.33983 | valid_logloss: 0.34572 |  0:00:12s\n","epoch 4  | loss: 0.29922 | train_logloss: 0.32384 | valid_logloss: 0.3297  |  0:00:15s\n","epoch 5  | loss: 0.29779 | train_logloss: 0.31127 | valid_logloss: 0.31703 |  0:00:18s\n","epoch 6  | loss: 0.29714 | train_logloss: 0.3077  | valid_logloss: 0.31611 |  0:00:21s\n","epoch 7  | loss: 0.29544 | train_logloss: 0.29693 | valid_logloss: 0.30571 |  0:00:23s\n","epoch 8  | loss: 0.29422 | train_logloss: 0.29548 | valid_logloss: 0.30471 |  0:00:26s\n","epoch 9  | loss: 0.2948  | train_logloss: 0.29287 | valid_logloss: 0.3014  |  0:00:28s\n","epoch 10 | loss: 0.29484 | train_logloss: 0.29152 | valid_logloss: 0.30022 |  0:00:32s\n","epoch 11 | loss: 0.29386 | train_logloss: 0.29312 | valid_logloss: 0.30158 |  0:00:34s\n","epoch 12 | loss: 0.29309 | train_logloss: 0.29001 | valid_logloss: 0.30055 |  0:00:36s\n","epoch 13 | loss: 0.29083 | train_logloss: 0.29093 | valid_logloss: 0.30185 |  0:00:39s\n","epoch 14 | loss: 0.29186 | train_logloss: 0.29127 | valid_logloss: 0.30271 |  0:00:41s\n","epoch 15 | loss: 0.29144 | train_logloss: 0.28848 | valid_logloss: 0.29997 |  0:00:45s\n","epoch 16 | loss: 0.29219 | train_logloss: 0.28984 | valid_logloss: 0.30027 |  0:00:47s\n","epoch 17 | loss: 0.2918  | train_logloss: 0.28851 | valid_logloss: 0.29938 |  0:00:50s\n","epoch 18 | loss: 0.29083 | train_logloss: 0.28918 | valid_logloss: 0.29999 |  0:00:52s\n","epoch 19 | loss: 0.29174 | train_logloss: 0.28873 | valid_logloss: 0.29877 |  0:00:55s\n","epoch 20 | loss: 0.29129 | train_logloss: 0.28787 | valid_logloss: 0.29947 |  0:00:58s\n","epoch 21 | loss: 0.29222 | train_logloss: 0.28878 | valid_logloss: 0.2988  |  0:01:01s\n","epoch 22 | loss: 0.29158 | train_logloss: 0.28826 | valid_logloss: 0.30048 |  0:01:03s\n","epoch 23 | loss: 0.29042 | train_logloss: 0.28741 | valid_logloss: 0.29791 |  0:01:06s\n","epoch 24 | loss: 0.29046 | train_logloss: 0.29436 | valid_logloss: 0.30586 |  0:01:08s\n","epoch 25 | loss: 0.29037 | train_logloss: 0.28753 | valid_logloss: 0.29943 |  0:01:11s\n","epoch 26 | loss: 0.28997 | train_logloss: 0.28624 | valid_logloss: 0.29746 |  0:01:14s\n","epoch 27 | loss: 0.28977 | train_logloss: 0.28633 | valid_logloss: 0.29852 |  0:01:16s\n","epoch 28 | loss: 0.29172 | train_logloss: 0.28851 | valid_logloss: 0.29992 |  0:01:19s\n","epoch 29 | loss: 0.28902 | train_logloss: 0.29106 | valid_logloss: 0.30201 |  0:01:21s\n","epoch 30 | loss: 0.2928  | train_logloss: 0.28858 | valid_logloss: 0.2997  |  0:01:24s\n","epoch 31 | loss: 0.29129 | train_logloss: 0.28901 | valid_logloss: 0.30007 |  0:01:27s\n","epoch 32 | loss: 0.2901  | train_logloss: 0.28816 | valid_logloss: 0.30029 |  0:01:29s\n","epoch 33 | loss: 0.29121 | train_logloss: 0.29118 | valid_logloss: 0.30203 |  0:01:32s\n","epoch 34 | loss: 0.2903  | train_logloss: 0.28692 | valid_logloss: 0.29731 |  0:01:35s\n","epoch 35 | loss: 0.28943 | train_logloss: 0.28755 | valid_logloss: 0.29874 |  0:01:38s\n","epoch 36 | loss: 0.28958 | train_logloss: 0.28708 | valid_logloss: 0.29893 |  0:01:41s\n","epoch 37 | loss: 0.29077 | train_logloss: 0.28806 | valid_logloss: 0.29902 |  0:01:43s\n","epoch 38 | loss: 0.29099 | train_logloss: 0.2878  | valid_logloss: 0.29952 |  0:01:46s\n","epoch 39 | loss: 0.29011 | train_logloss: 0.28701 | valid_logloss: 0.29806 |  0:01:49s\n","epoch 40 | loss: 0.29038 | train_logloss: 0.28631 | valid_logloss: 0.29759 |  0:01:52s\n","epoch 41 | loss: 0.28988 | train_logloss: 0.2865  | valid_logloss: 0.29804 |  0:01:54s\n","epoch 42 | loss: 0.28911 | train_logloss: 0.28698 | valid_logloss: 0.29972 |  0:01:57s\n","epoch 43 | loss: 0.28861 | train_logloss: 0.28628 | valid_logloss: 0.29746 |  0:01:59s\n","epoch 44 | loss: 0.2883  | train_logloss: 0.28531 | valid_logloss: 0.29728 |  0:02:02s\n","epoch 45 | loss: 0.28784 | train_logloss: 0.28536 | valid_logloss: 0.29696 |  0:02:05s\n","epoch 46 | loss: 0.28827 | train_logloss: 0.28592 | valid_logloss: 0.29806 |  0:02:08s\n","epoch 47 | loss: 0.28911 | train_logloss: 0.28484 | valid_logloss: 0.2974  |  0:02:10s\n","epoch 48 | loss: 0.28768 | train_logloss: 0.28463 | valid_logloss: 0.29706 |  0:02:13s\n","epoch 49 | loss: 0.28766 | train_logloss: 0.28522 | valid_logloss: 0.29752 |  0:02:16s\n","epoch 50 | loss: 0.28989 | train_logloss: 0.28596 | valid_logloss: 0.29886 |  0:02:19s\n","epoch 51 | loss: 0.28886 | train_logloss: 0.28525 | valid_logloss: 0.29833 |  0:02:22s\n","epoch 52 | loss: 0.28687 | train_logloss: 0.28416 | valid_logloss: 0.29859 |  0:02:24s\n","epoch 53 | loss: 0.28597 | train_logloss: 0.28558 | valid_logloss: 0.29872 |  0:02:27s\n","epoch 54 | loss: 0.28898 | train_logloss: 0.28484 | valid_logloss: 0.29669 |  0:02:30s\n","epoch 55 | loss: 0.28885 | train_logloss: 0.28459 | valid_logloss: 0.29848 |  0:02:33s\n","epoch 56 | loss: 0.28766 | train_logloss: 0.28585 | valid_logloss: 0.299   |  0:02:35s\n","epoch 57 | loss: 0.28716 | train_logloss: 0.28407 | valid_logloss: 0.29724 |  0:02:38s\n","epoch 58 | loss: 0.28689 | train_logloss: 0.28395 | valid_logloss: 0.29798 |  0:02:40s\n","epoch 59 | loss: 0.28947 | train_logloss: 0.28481 | valid_logloss: 0.29796 |  0:02:43s\n","epoch 60 | loss: 0.28787 | train_logloss: 0.28616 | valid_logloss: 0.2989  |  0:02:46s\n","epoch 61 | loss: 0.28622 | train_logloss: 0.28298 | valid_logloss: 0.29631 |  0:02:49s\n","epoch 62 | loss: 0.28686 | train_logloss: 0.28755 | valid_logloss: 0.304   |  0:02:51s\n","epoch 63 | loss: 0.28721 | train_logloss: 0.28662 | valid_logloss: 0.29983 |  0:02:53s\n","epoch 64 | loss: 0.28868 | train_logloss: 0.28431 | valid_logloss: 0.29814 |  0:02:56s\n","epoch 65 | loss: 0.28788 | train_logloss: 0.28365 | valid_logloss: 0.2963  |  0:02:59s\n","epoch 66 | loss: 0.28687 | train_logloss: 0.28413 | valid_logloss: 0.29822 |  0:03:02s\n","epoch 67 | loss: 0.28589 | train_logloss: 0.28311 | valid_logloss: 0.29646 |  0:03:04s\n","epoch 68 | loss: 0.28661 | train_logloss: 0.28446 | valid_logloss: 0.29887 |  0:03:07s\n","epoch 69 | loss: 0.28784 | train_logloss: 0.28324 | valid_logloss: 0.30015 |  0:03:10s\n","epoch 70 | loss: 0.28604 | train_logloss: 0.28311 | valid_logloss: 0.29724 |  0:03:13s\n","epoch 71 | loss: 0.28647 | train_logloss: 0.28299 | valid_logloss: 0.29603 |  0:03:15s\n","epoch 72 | loss: 0.287   | train_logloss: 0.28514 | valid_logloss: 0.30037 |  0:03:17s\n","epoch 73 | loss: 0.28677 | train_logloss: 0.2832  | valid_logloss: 0.29938 |  0:03:20s\n","epoch 74 | loss: 0.28692 | train_logloss: 0.283   | valid_logloss: 0.29872 |  0:03:24s\n","epoch 75 | loss: 0.28497 | train_logloss: 0.28169 | valid_logloss: 0.29741 |  0:03:26s\n","epoch 76 | loss: 0.28451 | train_logloss: 0.28417 | valid_logloss: 0.29975 |  0:03:29s\n","epoch 77 | loss: 0.28573 | train_logloss: 0.28461 | valid_logloss: 0.29991 |  0:03:31s\n","epoch 78 | loss: 0.28575 | train_logloss: 0.28363 | valid_logloss: 0.30015 |  0:03:33s\n","epoch 79 | loss: 0.28629 | train_logloss: 0.28438 | valid_logloss: 0.2988  |  0:03:37s\n","epoch 80 | loss: 0.28567 | train_logloss: 0.28327 | valid_logloss: 0.29891 |  0:03:40s\n","epoch 81 | loss: 0.28675 | train_logloss: 0.28269 | valid_logloss: 0.29591 |  0:03:43s\n","epoch 82 | loss: 0.28465 | train_logloss: 0.28342 | valid_logloss: 0.29815 |  0:03:45s\n","epoch 83 | loss: 0.28598 | train_logloss: 0.28347 | valid_logloss: 0.30052 |  0:03:48s\n","epoch 84 | loss: 0.28587 | train_logloss: 0.29136 | valid_logloss: 0.30612 |  0:03:51s\n","epoch 85 | loss: 0.2853  | train_logloss: 0.28264 | valid_logloss: 0.29758 |  0:03:54s\n","epoch 86 | loss: 0.28398 | train_logloss: 0.28172 | valid_logloss: 0.29938 |  0:03:56s\n","epoch 87 | loss: 0.28301 | train_logloss: 0.28113 | valid_logloss: 0.2988  |  0:03:58s\n","epoch 88 | loss: 0.28399 | train_logloss: 0.28148 | valid_logloss: 0.29884 |  0:04:01s\n","epoch 89 | loss: 0.28334 | train_logloss: 0.28314 | valid_logloss: 0.29999 |  0:04:04s\n","epoch 90 | loss: 0.28534 | train_logloss: 0.28151 | valid_logloss: 0.29849 |  0:04:07s\n","epoch 91 | loss: 0.28472 | train_logloss: 0.28396 | valid_logloss: 0.29859 |  0:04:09s\n","\n","Early stopping occurred at epoch 91 with best_epoch = 81 and best_valid_logloss = 0.29591\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]},{"output_type":"stream","name":"stdout","text":["fold 3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 3.664   | val_0_unsup_loss_numpy: 2.549499988555908|  0:00:01s\n","epoch 1  | loss: 1.41366 | val_0_unsup_loss_numpy: 1.1893500089645386|  0:00:03s\n","epoch 2  | loss: 0.99669 | val_0_unsup_loss_numpy: 1.0238399505615234|  0:00:06s\n","epoch 3  | loss: 0.98819 | val_0_unsup_loss_numpy: 0.9657800197601318|  0:00:09s\n","epoch 4  | loss: 0.97443 | val_0_unsup_loss_numpy: 0.9454799890518188|  0:00:10s\n","epoch 5  | loss: 0.97241 | val_0_unsup_loss_numpy: 0.9264000058174133|  0:00:12s\n","epoch 6  | loss: 0.96694 | val_0_unsup_loss_numpy: 0.9080399870872498|  0:00:14s\n","epoch 7  | loss: 0.96195 | val_0_unsup_loss_numpy: 0.8921399712562561|  0:00:16s\n","epoch 8  | loss: 0.95106 | val_0_unsup_loss_numpy: 0.8582500219345093|  0:00:19s\n","epoch 9  | loss: 0.94926 | val_0_unsup_loss_numpy: 0.8512099981307983|  0:00:22s\n","epoch 10 | loss: 0.94057 | val_0_unsup_loss_numpy: 0.8219699859619141|  0:00:23s\n","epoch 11 | loss: 0.93551 | val_0_unsup_loss_numpy: 0.8164399862289429|  0:00:25s\n","epoch 12 | loss: 0.93681 | val_0_unsup_loss_numpy: 0.8208799958229065|  0:00:27s\n","epoch 13 | loss: 0.955   | val_0_unsup_loss_numpy: 0.8251199722290039|  0:00:29s\n","epoch 14 | loss: 0.93551 | val_0_unsup_loss_numpy: 0.8031200170516968|  0:00:31s\n","epoch 15 | loss: 0.93268 | val_0_unsup_loss_numpy: 0.7958800196647644|  0:00:34s\n","epoch 16 | loss: 0.93534 | val_0_unsup_loss_numpy: 0.7967399954795837|  0:00:37s\n","epoch 17 | loss: 0.93693 | val_0_unsup_loss_numpy: 0.8001499772071838|  0:00:39s\n","epoch 18 | loss: 0.93356 | val_0_unsup_loss_numpy: 0.803820013999939|  0:00:41s\n","epoch 19 | loss: 0.93847 | val_0_unsup_loss_numpy: 0.7937800288200378|  0:00:43s\n","epoch 20 | loss: 0.94594 | val_0_unsup_loss_numpy: 0.8054400086402893|  0:00:45s\n","epoch 21 | loss: 0.94001 | val_0_unsup_loss_numpy: 0.7949900031089783|  0:00:48s\n","epoch 22 | loss: 0.9357  | val_0_unsup_loss_numpy: 0.8005800247192383|  0:00:50s\n","epoch 23 | loss: 0.93324 | val_0_unsup_loss_numpy: 0.791920006275177|  0:00:52s\n","epoch 24 | loss: 0.9346  | val_0_unsup_loss_numpy: 0.7935000061988831|  0:00:54s\n","epoch 25 | loss: 0.94342 | val_0_unsup_loss_numpy: 0.7875300049781799|  0:00:56s\n","epoch 26 | loss: 0.92434 | val_0_unsup_loss_numpy: 0.7924299836158752|  0:00:58s\n","epoch 27 | loss: 0.93074 | val_0_unsup_loss_numpy: 0.7922199964523315|  0:01:01s\n","epoch 28 | loss: 0.92291 | val_0_unsup_loss_numpy: 0.8041700124740601|  0:01:03s\n","epoch 29 | loss: 0.91876 | val_0_unsup_loss_numpy: 0.7883700132369995|  0:01:05s\n","epoch 30 | loss: 0.92521 | val_0_unsup_loss_numpy: 0.7906200289726257|  0:01:07s\n","epoch 31 | loss: 0.93536 | val_0_unsup_loss_numpy: 0.7769500017166138|  0:01:09s\n","epoch 32 | loss: 0.92584 | val_0_unsup_loss_numpy: 0.7904599905014038|  0:01:11s\n","epoch 33 | loss: 0.92305 | val_0_unsup_loss_numpy: 0.7810099720954895|  0:01:14s\n","epoch 34 | loss: 0.93321 | val_0_unsup_loss_numpy: 0.7907699942588806|  0:01:16s\n","epoch 35 | loss: 0.92376 | val_0_unsup_loss_numpy: 0.7816900014877319|  0:01:18s\n","epoch 36 | loss: 0.9296  | val_0_unsup_loss_numpy: 0.7746400237083435|  0:01:20s\n","epoch 37 | loss: 0.92504 | val_0_unsup_loss_numpy: 0.7890999913215637|  0:01:22s\n","epoch 38 | loss: 0.91734 | val_0_unsup_loss_numpy: 0.7900800108909607|  0:01:24s\n","epoch 39 | loss: 0.91604 | val_0_unsup_loss_numpy: 0.7765100002288818|  0:01:26s\n","epoch 40 | loss: 0.91372 | val_0_unsup_loss_numpy: 0.7748200297355652|  0:01:29s\n","epoch 41 | loss: 0.92008 | val_0_unsup_loss_numpy: 0.7874900102615356|  0:01:31s\n","epoch 42 | loss: 0.91843 | val_0_unsup_loss_numpy: 0.7614200115203857|  0:01:33s\n","epoch 43 | loss: 0.91958 | val_0_unsup_loss_numpy: 0.7684999704360962|  0:01:35s\n","epoch 44 | loss: 0.9113  | val_0_unsup_loss_numpy: 0.7529199719429016|  0:01:37s\n","epoch 45 | loss: 0.914   | val_0_unsup_loss_numpy: 0.7517899870872498|  0:01:40s\n","epoch 46 | loss: 0.90015 | val_0_unsup_loss_numpy: 0.748199999332428|  0:01:42s\n","epoch 47 | loss: 0.899   | val_0_unsup_loss_numpy: 0.7260800004005432|  0:01:44s\n","epoch 48 | loss: 0.89661 | val_0_unsup_loss_numpy: 0.7271999716758728|  0:01:46s\n","epoch 49 | loss: 0.9084  | val_0_unsup_loss_numpy: 0.717519998550415|  0:01:48s\n","epoch 50 | loss: 0.89645 | val_0_unsup_loss_numpy: 0.7047299742698669|  0:01:50s\n","epoch 51 | loss: 0.89614 | val_0_unsup_loss_numpy: 0.7312999963760376|  0:01:53s\n","epoch 52 | loss: 0.90456 | val_0_unsup_loss_numpy: 0.7226700186729431|  0:01:55s\n","epoch 53 | loss: 0.90215 | val_0_unsup_loss_numpy: 0.7078499794006348|  0:01:58s\n","epoch 54 | loss: 0.89935 | val_0_unsup_loss_numpy: 0.7289100289344788|  0:02:00s\n","epoch 55 | loss: 0.89584 | val_0_unsup_loss_numpy: 0.7367299795150757|  0:02:02s\n","epoch 56 | loss: 0.88599 | val_0_unsup_loss_numpy: 0.7169700264930725|  0:02:04s\n","epoch 57 | loss: 0.8996  | val_0_unsup_loss_numpy: 0.7061399817466736|  0:02:06s\n","epoch 58 | loss: 0.89198 | val_0_unsup_loss_numpy: 0.7068799734115601|  0:02:09s\n","epoch 59 | loss: 0.88647 | val_0_unsup_loss_numpy: 0.7051600217819214|  0:02:11s\n","epoch 60 | loss: 0.88905 | val_0_unsup_loss_numpy: 0.710099995136261|  0:02:13s\n","\n","Early stopping occurred at epoch 60 with best_epoch = 50 and best_val_0_unsup_loss_numpy = 0.7047299742698669\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n","  warnings.warn(f\"Device used : {self.device}\")\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_dims changed from [5, 7, 24, 38, 51, 52, 3] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_emb_dim changed from [1, 1, 1, 1, 1, 1, 1] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:118: UserWarning: Pretraining: cat_idxs changed from [3, 4, 5, 6, 7, 8, 9] to []\n","  warnings.warn(wrn_msg)\n","/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n","  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"]},{"output_type":"stream","name":"stdout","text":["epoch 0  | loss: 0.53279 | train_logloss: 0.41406 | valid_logloss: 0.41376 |  0:00:02s\n","epoch 1  | loss: 0.32755 | train_logloss: 0.3997  | valid_logloss: 0.39972 |  0:00:04s\n","epoch 2  | loss: 0.31809 | train_logloss: 0.36013 | valid_logloss: 0.36132 |  0:00:08s\n","epoch 3  | loss: 0.31273 | train_logloss: 0.34692 | valid_logloss: 0.34593 |  0:00:10s\n","epoch 4  | loss: 0.30509 | train_logloss: 0.32104 | valid_logloss: 0.32193 |  0:00:12s\n","epoch 5  | loss: 0.30358 | train_logloss: 0.31639 | valid_logloss: 0.31666 |  0:00:15s\n","epoch 6  | loss: 0.30286 | train_logloss: 0.30305 | valid_logloss: 0.30275 |  0:00:17s\n","epoch 7  | loss: 0.30269 | train_logloss: 0.3002  | valid_logloss: 0.299   |  0:00:21s\n","epoch 8  | loss: 0.30058 | train_logloss: 0.2965  | valid_logloss: 0.2951  |  0:00:23s\n","epoch 9  | loss: 0.29901 | train_logloss: 0.29963 | valid_logloss: 0.29905 |  0:00:25s\n","epoch 10 | loss: 0.29895 | train_logloss: 0.29559 | valid_logloss: 0.29497 |  0:00:28s\n","epoch 11 | loss: 0.29883 | train_logloss: 0.2952  | valid_logloss: 0.29295 |  0:00:30s\n","epoch 12 | loss: 0.29813 | train_logloss: 0.29339 | valid_logloss: 0.29279 |  0:00:34s\n","epoch 13 | loss: 0.29736 | train_logloss: 0.29471 | valid_logloss: 0.29296 |  0:00:36s\n","epoch 14 | loss: 0.29587 | train_logloss: 0.2938  | valid_logloss: 0.29159 |  0:00:38s\n","epoch 15 | loss: 0.29682 | train_logloss: 0.2946  | valid_logloss: 0.29312 |  0:00:41s\n","epoch 16 | loss: 0.29552 | train_logloss: 0.29296 | valid_logloss: 0.29174 |  0:00:44s\n","epoch 17 | loss: 0.29723 | train_logloss: 0.29543 | valid_logloss: 0.29391 |  0:00:47s\n","epoch 18 | loss: 0.29741 | train_logloss: 0.29338 | valid_logloss: 0.29314 |  0:00:50s\n","epoch 19 | loss: 0.29567 | train_logloss: 0.29377 | valid_logloss: 0.29301 |  0:00:52s\n","epoch 20 | loss: 0.29633 | train_logloss: 0.29247 | valid_logloss: 0.29333 |  0:00:54s\n","epoch 21 | loss: 0.29498 | train_logloss: 0.29243 | valid_logloss: 0.29099 |  0:00:57s\n","epoch 22 | loss: 0.29534 | train_logloss: 0.29486 | valid_logloss: 0.29361 |  0:01:01s\n","epoch 23 | loss: 0.29476 | train_logloss: 0.29159 | valid_logloss: 0.29046 |  0:01:03s\n","epoch 24 | loss: 0.29471 | train_logloss: 0.29183 | valid_logloss: 0.29072 |  0:01:05s\n","epoch 25 | loss: 0.29524 | train_logloss: 0.29317 | valid_logloss: 0.29196 |  0:01:08s\n","epoch 26 | loss: 0.2954  | train_logloss: 0.29167 | valid_logloss: 0.29049 |  0:01:10s\n","epoch 27 | loss: 0.29442 | train_logloss: 0.29113 | valid_logloss: 0.28971 |  0:01:14s\n","epoch 28 | loss: 0.29473 | train_logloss: 0.29276 | valid_logloss: 0.29105 |  0:01:16s\n","epoch 29 | loss: 0.29536 | train_logloss: 0.29194 | valid_logloss: 0.29121 |  0:01:19s\n","epoch 30 | loss: 0.294   | train_logloss: 0.29292 | valid_logloss: 0.29166 |  0:01:21s\n","epoch 31 | loss: 0.29426 | train_logloss: 0.29212 | valid_logloss: 0.29034 |  0:01:24s\n","epoch 32 | loss: 0.29373 | train_logloss: 0.29037 | valid_logloss: 0.28852 |  0:01:27s\n","epoch 33 | loss: 0.29311 | train_logloss: 0.29105 | valid_logloss: 0.28938 |  0:01:30s\n","epoch 34 | loss: 0.29395 | train_logloss: 0.29045 | valid_logloss: 0.28926 |  0:01:32s\n","epoch 35 | loss: 0.29271 | train_logloss: 0.29119 | valid_logloss: 0.29134 |  0:01:35s\n","epoch 36 | loss: 0.29458 | train_logloss: 0.2921  | valid_logloss: 0.29051 |  0:01:38s\n","epoch 37 | loss: 0.29453 | train_logloss: 0.2916  | valid_logloss: 0.29002 |  0:01:41s\n","epoch 38 | loss: 0.29372 | train_logloss: 0.29105 | valid_logloss: 0.28992 |  0:01:43s\n","epoch 39 | loss: 0.29219 | train_logloss: 0.29064 | valid_logloss: 0.28909 |  0:01:46s\n","epoch 40 | loss: 0.29418 | train_logloss: 0.29155 | valid_logloss: 0.28988 |  0:01:48s\n","epoch 41 | loss: 0.29271 | train_logloss: 0.2903  | valid_logloss: 0.28982 |  0:01:51s\n","epoch 42 | loss: 0.29379 | train_logloss: 0.29088 | valid_logloss: 0.29036 |  0:01:54s\n","\n","Early stopping occurred at epoch 42 with best_epoch = 32 and best_valid_logloss = 0.28852\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n","  warnings.warn(wrn_msg)\n"]}],"source":["col_target = \"MIS_Status\"\n","\n","# train tabnet\n","clf_lst, oof_pred = train_tabnet(train, cols_exp, cols_cat, col_target)\n","\n","# normalization for test\n","x_train = train[cols_exp].to_numpy()\n","x_test = test[cols_exp].to_numpy()\n","cols_num_idxs = [i for i, c in enumerate(cols_exp) if not c in cols_cat]\n","\n","# cols_num_idxsが空でない場合にのみ実行\n","if cols_num_idxs:  # cols_num_idxsが空でない場合にのみ実行\n","  scaler = StandardScaler()\n","  scaler.fit(x_train[:, cols_num_idxs])\n","  x_test[:, cols_num_idxs] = scaler.transform(x_test[:, cols_num_idxs])\n","\n","# predict test with CV ensemble\n","y_test_pred = predict_test(x_test, clf_lst)\n","\n","# record\n","oof_pred_df = pd.DataFrame(oof_pred, columns=[f\"MIS_Status_{h}\" for h in range(2)])\n","test_pred_df = pd.DataFrame(y_test_pred, columns=[f\"MIS_Status_{h}\" for h in range(2)])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFuqVtmh66Mq","executionInfo":{"status":"ok","timestamp":1707923194589,"user_tz":-540,"elapsed":13,"user":{"displayName":"田口采樹","userId":"02806376211526518224"}},"outputId":"e33407e0-5547-4f6b-f23c-70bdd3545017"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean F1 Score (Macro): 0.62419393785348\n"]}],"source":["from sklearn.metrics import f1_score\n","\n","# 実際のターゲット値\n","y_true = train[col_target].values\n","\n","# OOF予測値をクラスに変換（確率から最も高いクラスを選択）\n","# 二値分類の場合、0.5を閾値としてクラスを決定することが一般的です\n","y_oof_pred_class = (oof_pred[:, 1] > 0.5).astype(int)\n","\n","# mean F1スコア（マクロ平均）を計算\n","mean_f1_score_macro = f1_score(y_true, y_oof_pred_class, average='macro')\n","print(f'Mean F1 Score (Macro): {mean_f1_score_macro}')\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"yBV8m4Kyy8dN","executionInfo":{"status":"ok","timestamp":1707923195479,"user_tz":-540,"elapsed":901,"user":{"displayName":"田口采樹","userId":"02806376211526518224"}}},"outputs":[],"source":["# save\n","oof_pred_df.to_csv(f\"/content/drive/MyDrive/signate/債務不履行/model_feat/oof_pred_tabnet_{feat}.csv\", index=False)\n","test_pred_df.to_csv(f\"/content/drive/MyDrive/signate/債務不履行/model_feat/test_pred_tabnet_{feat}.csv\", index=False)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxhgSggqKmuS3ct1JflyvM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}